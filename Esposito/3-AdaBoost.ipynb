{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from copy import copy\n",
    "import sklearn.datasets\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "source": [
    "# Dataset\n",
    "\n",
    "We import the dataset from Hastie & Tibshirani book. \n",
    "This is an artificially generated binary classification problem. Labels are in the set $\\{-1,+1\\}$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = sklearn.datasets.make_hastie_10_2()\n",
    "X_train = X[0:8000,:]\n",
    "y_train = y[0:8000]\n",
    "X_test = X[8000:,:]\n",
    "y_test = y[8000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost implementation\n",
    "\n",
    "Here we implement the Adaboost algorith. We shall assume that:\n",
    "- that the problem is a binary classification problem with labels in $\\{-1, +1\\}$.\n",
    "- that the weakModel can fit a weighted sample set by means of the call `weakModel.fit(X,y,sample_weight=w)` where `w` is a vector of length $n=|X|=|y|$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost:\n",
    "    def __init__(self, weakModel, T):\n",
    "        return None # to be completed\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return None # to be completed\n",
    "\n",
    "    def predict(self, X):\n",
    "        return None # to be completed"
   ]
  },
  {
   "source": [
    "# Testing with an SVM"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now see how our iplementation of AdaBoost performs on the dataset we loaded above. In this experiment we want the weak learning algorithm $\\mathcal{A}$ to be good, but not too much. An SVM with a polynomial kernel of degre 3 works fine for our needs.\n",
    "\n",
    "The SVC implementation provided by sklearn does not work well when weights are normalized. The following code simply \"denormalize\" weights befor calling into SVC implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    class SVC_:\n",
    "        def __init__(self, kernel=\"rbf\", degree=\"3\"):\n",
    "            self.svc = SVC(kernel=kernel, degree=degree)\n",
    "\n",
    "        def fit(self, X,y,sample_weight=None):\n",
    "            if sample_weight is not None:\n",
    "                sample_weight = sample_weight * len(X)\n",
    "\n",
    "            self.svc.fit(X,y,sample_weight=sample_weight)\n",
    "            return self\n",
    "\n",
    "        def predict(self, X):\n",
    "            return self.svc.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weakModel = SVC(kernel=\"poly\", degree=3)\n",
    "adaboost = AdaBoost(weakModel, 100)\n",
    "y_train_ = c.predict(X_train)\n",
    "y_test_ = c.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (0.5 - y_train_.T * y_train/(2 * len(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on the weakest of the weak learners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to experiment with a VERY weak learner. The weak learner works as follows:\n",
    "\n",
    "- it creates a random linear model by generating the needed weight vector $\\mathbf{w}$ at random; each weight shall be sampled from U(-1,1);\n",
    "- it evaluates the weighted loss $\\epsilon_t$ on the given dataset and flip the linear model if $\\epsilon_t > 0.5$\n",
    "- at prediction time it predicts +1 if $\\mathbf{x} \\cdot \\mathbf{w} > 0$ it predicts -1 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomLinearModel:\n",
    "    def loss(self, y, y_, w):\n",
    "        return None # to be completed\n",
    "        \n",
    "    def fit(self,X,y,sample_weight=None):\n",
    "        return None # to be completed        \n",
    "        \n",
    "    def predict(self,X):\n",
    "        return None # to be completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now learn an AdaBoost model using the RandomLinearModel weak learner printing every $K$ iterations the weighted error and the current error of the ensemble. Evaluate the training and test error of the final ensemble model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = RandomLinearModel()\n",
    "a = AdaBoost(rs,10000)\n",
    "a.fit(X_train,y_train)\n",
    "\n",
    "y_train_ = a.predict(X_train)\n",
    "y_test_ = a.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}